{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# This is a sample Jupyter Notebook\n",
    "\n",
    "Below is an example of a code cell. \n",
    "Put your cursor into the cell and press Shift+Enter to execute it and select the next one, or click 'Run Cell' button.\n",
    "\n",
    "Press Double Shift to search everywhere for classes, files, tool windows, actions, and settings.\n",
    "\n",
    "To learn more about Jupyter Notebooks in PyCharm, see [help](https://www.jetbrains.com/help/pycharm/ipython-notebook-support.html).\n",
    "For an overview of PyCharm, go to Help -> Learn IDE features or refer to [our documentation](https://www.jetbrains.com/help/pycharm/getting-started.html)."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T19:57:22.531504Z",
     "start_time": "2025-05-07T19:57:22.514312Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"Hello World!\")\n",
   "id": "fbc121e30a2defb3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T19:57:25.785049Z",
     "start_time": "2025-05-07T19:57:25.724826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    raw_text = file.read()\n",
    "\n",
    "print(\"Total number of characters: \", len(raw_text))\n",
    "print(raw_text[:99])"
   ],
   "id": "17cbdd9a51ca1067",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters:  20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T19:57:29.549111Z",
     "start_time": "2025-05-07T19:57:29.507353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "print(len(raw_text))\n",
    "# Split based on punctuation and whitespace\n",
    "preprocess = re.split(r'([,.:;?_!\"\\']|--|\\s)', raw_text)\n",
    "\n",
    "# Remove empty strings and whitespace-only tokens\n",
    "preprocess = [item.strip() for item in preprocess if item.strip()]\n",
    "\n",
    "# Print the first 30 tokens\n",
    "print(preprocess[:30])\n"
   ],
   "id": "50018a4e6b8a0f7d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20479\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T19:57:34.043711Z",
     "start_time": "2025-05-07T19:57:34.030026Z"
    }
   },
   "cell_type": "code",
   "source": "print(len(preprocess))",
   "id": "1c8b5cd22f00f96a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4685\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T19:57:58.180437Z",
     "start_time": "2025-05-07T19:57:58.169110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# let remove the duplicate from the list\n",
    "all_words=sorted(set(preprocess))\n",
    "vocab_size=len(all_words)\n",
    "print(vocab_size)\n"
   ],
   "id": "3ae5f369faff43c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1131\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T19:58:02.212362Z",
     "start_time": "2025-05-07T19:58:02.204765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# assign the token using the enumerate function\n",
    "all_words=sorted(set(preprocess))\n",
    "vocab={token:integer for integer, token in enumerate(all_words)}\n",
    "\n"
   ],
   "id": "f5280ecd5609803c",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T19:58:03.706326Z",
     "start_time": "2025-05-07T19:58:03.696088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i , item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i>50:\n",
    "        break"
   ],
   "id": "e4d0d93e34bbbf73",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(I', 3)\n",
      "('(Though', 4)\n",
      "(')', 5)\n",
      "(',', 6)\n",
      "('--', 7)\n",
      "('.', 8)\n",
      "(':', 9)\n",
      "(';', 10)\n",
      "('?', 11)\n",
      "('A', 12)\n",
      "('Ah', 13)\n",
      "('Among', 14)\n",
      "('And', 15)\n",
      "('Are', 16)\n",
      "('Arrt', 17)\n",
      "('As', 18)\n",
      "('At', 19)\n",
      "('Be', 20)\n",
      "('Begin', 21)\n",
      "('Burlington', 22)\n",
      "('But', 23)\n",
      "('By', 24)\n",
      "('Carlo', 25)\n",
      "('Chicago', 26)\n",
      "('Claude', 27)\n",
      "('Come', 28)\n",
      "('Croft', 29)\n",
      "('Croft)', 30)\n",
      "('Destroyed', 31)\n",
      "('Devonshire', 32)\n",
      "('Don', 33)\n",
      "('Dubarry', 34)\n",
      "('Emperors', 35)\n",
      "('Florence', 36)\n",
      "('For', 37)\n",
      "('Gallery', 38)\n",
      "('Gideon', 39)\n",
      "('Gisburn', 40)\n",
      "('Gisburns', 41)\n",
      "('Grafton', 42)\n",
      "('Greek', 43)\n",
      "('Grindle', 44)\n",
      "('Grindles', 45)\n",
      "('HAD', 46)\n",
      "('Had', 47)\n",
      "('Hang', 48)\n",
      "('Has', 49)\n",
      "('He', 50)\n",
      "('Her', 51)\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T19:58:05.494493Z",
     "start_time": "2025-05-07T19:58:05.476785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i: s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int.get(s, -1) for s in preprocessed]  # Use -1 for unknown tokens\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str.get(i, \"<UNK>\") for i in ids])\n",
    "        text = re.sub(r'\\s([,.:;?_!\"\\'])', r'\\1', text)  # Remove space before punctuation\n",
    "        return text\n",
    "\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text=\"\"\"I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that,\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(\"Encoded:\", ids)  # âžœ [1, 2, 3, 4]"
   ],
   "id": "a9f49d5828c0370d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [55, 46, 150, 1004, 59, 40, 819, 116, 257, 487, 7, 1003, 116, 501, 436, 393, 7, 909, 586, 1078, 710, 509, 962, 1017, 664, 1017, 536, 988, 6]\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T19:58:07.038045Z",
     "start_time": "2025-05-07T19:58:07.000419Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.decode(ids)",
   "id": "3d7fb1b791fca751",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I HAD always thought Jack Gisburn rather a cheap genius -- though a good fellow enough -- so it was no great surprise to me to hear that,'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T19:58:09.142554Z",
     "start_time": "2025-05-07T19:58:09.135869Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# special token\n",
    "\n",
    "all_words=sorted(list(set(preprocess)))\n",
    "all_words.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab={token:integer for integer, token in enumerate(all_words)}\n"
   ],
   "id": "416e5d61c8a6b188",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T19:58:12.633784Z",
     "start_time": "2025-05-07T19:58:12.619640Z"
    }
   },
   "cell_type": "code",
   "source": "print(len(vocab))",
   "id": "4294a5683ff8c495",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1133\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T19:58:13.631482Z",
     "start_time": "2025-05-07T19:58:13.613052Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ],
   "id": "da302db5470ae4a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1128)\n",
      "('your', 1129)\n",
      "('yourself', 1130)\n",
      "('<|endoftext|>', 1131)\n",
      "('<|unk|>', 1132)\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T20:12:19.886171Z",
     "start_time": "2025-05-07T20:12:19.839122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i: s for s, i in vocab.items()}\n",
    "        self.unknown_token = \"<|unk|>\"\n",
    "        self.unknown_token_id = self.str_to_int.get(self.unknown_token, -1)\n",
    "\n",
    "    def encode(self, text):\n",
    "        if not text:\n",
    "            return []\n",
    "\n",
    "        # Split into tokens including punctuation and whitespace\n",
    "        preprocessed = re.split(r'([,.:;?_!\"\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "        # Convert tokens to IDs, unknown tokens get <|unk|> id\n",
    "        ids = [self.str_to_int.get(s, self.unknown_token_id) for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        if not ids:\n",
    "            return \"\"\n",
    "\n",
    "        # Convert IDs to tokens\n",
    "        tokens = [self.int_to_str.get(i, self.unknown_token) for i in ids]\n",
    "\n",
    "        # Join and fix punctuation spacing\n",
    "        text = \" \".join(tokens)\n",
    "        text = re.sub(r'\\s([,.:;?_!\"\\'])', r'\\1', text)\n",
    "        return text\n",
    "\n",
    "\n",
    "# Example text\n",
    "text = \"\"\"I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that,\"\"\"\n",
    "\n",
    "# Preprocess to extract all unique words/tokens for vocab\n",
    "preprocess = re.split(r'([,.:;?_!\"\\']|--|\\s)', text)\n",
    "preprocess = [item.strip() for item in preprocess if item.strip()]\n",
    "all_words = sorted(list(set(preprocess)))\n",
    "\n",
    "# Add special tokens\n",
    "all_words.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "# Create vocab dictionary\n",
    "vocab = {token: idx for idx, token in enumerate(all_words)}\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "# Encode & Decode\n",
    "ids = tokenizer.encode(text)\n",
    "decoded_text = tokenizer.decode(ids)\n",
    "\n",
    "print(\"Encoded IDs:\", ids)\n",
    "print(\"Decoded Text:\", decoded_text)\n"
   ],
   "id": "7cef30fb1db59d5b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded IDs: [4, 3, 7, 23, 5, 2, 18, 6, 8, 11, 1, 22, 6, 12, 10, 9, 1, 19, 15, 25, 17, 13, 20, 24, 16, 24, 14, 21, 0]\n",
      "Decoded Text: I HAD always thought Jack Gisburn rather a cheap genius -- though a good fellow enough -- so it was no great surprise to me to hear that,\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T20:13:39.145684Z",
     "start_time": "2025-05-07T20:13:39.129570Z"
    }
   },
   "cell_type": "code",
   "source": "# use of <endofetxt>",
   "id": "c7dc3dc863cbba63",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T20:13:44.181702Z",
     "start_time": "2025-05-07T20:13:44.140794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i: s for s, i in vocab.items()}\n",
    "        self.unknown_token = \"<|unk|>\"\n",
    "        self.endoftext_token = \"<|endoftext|>\"\n",
    "        self.unknown_token_id = self.str_to_int.get(self.unknown_token, -1)\n",
    "        self.endoftext_token_id = self.str_to_int.get(self.endoftext_token)\n",
    "\n",
    "    def encode(self, text):\n",
    "        if not text:\n",
    "            return []\n",
    "\n",
    "        preprocessed = re.split(r'([,.:;?_!\"\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int.get(s, self.unknown_token_id) for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        if not ids:\n",
    "            return \"\"\n",
    "        tokens = [self.int_to_str.get(i, self.unknown_token) for i in ids]\n",
    "        text = \" \".join(tokens)\n",
    "        text = re.sub(r'\\s([,.:;?_!\"\\'])', r'\\1', text)\n",
    "        return text\n",
    "\n",
    "# Input texts\n",
    "text1 = \"I HAD always thought Jack Gisburn rather a cheap genius.\"\n",
    "text2 = \"He was a good fellow enough, no great surprise to me.\"\n",
    "\n",
    "# Build vocabulary from both texts\n",
    "preprocess = re.split(r'([,.:;?_!\"\\']|--|\\s)', text1 + \" \" + text2)\n",
    "preprocess = [item.strip() for item in preprocess if item.strip()]\n",
    "all_words = sorted(list(set(preprocess)))\n",
    "all_words.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token: idx for idx, token in enumerate(all_words)}\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "# Encode both texts with <|endoftext|> in between\n",
    "ids1 = tokenizer.encode(text1)\n",
    "ids2 = tokenizer.encode(text2)\n",
    "combined_ids = ids1 + [tokenizer.endoftext_token_id] + ids2\n",
    "\n",
    "# Decode combined\n",
    "decoded = tokenizer.decode(combined_ids)\n",
    "\n",
    "print(\"Encoded IDs:\", combined_ids)\n",
    "print(\"Decoded Text:\", decoded)\n"
   ],
   "id": "b8eb364fa98f17d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded IDs: [5, 3, 8, 19, 6, 2, 17, 7, 9, 12, 1, 22, 4, 21, 7, 13, 11, 10, 0, 16, 14, 18, 20, 15, 1]\n",
      "Decoded Text: I HAD always thought Jack Gisburn rather a cheap genius. <|endoftext|> He was a good fellow enough, no great surprise to me.\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8b67515e22950b95"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
